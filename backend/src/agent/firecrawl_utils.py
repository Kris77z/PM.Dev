"""
V1.5 Firecrawlé›†æˆå·¥å…·
ä»V2ä¸­æå–æ ¸å¿ƒåŠŸèƒ½ï¼Œç®€åŒ–é›†æˆåˆ°V1
ä¸“ä¸ºV1.5ç‰ˆæœ¬è®¾è®¡çš„è½»é‡çº§Firecrawlå®¢æˆ·ç«¯
"""

# ç¡®ä¿åœ¨æ¨¡å—åŠ è½½æ—¶å°±åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv()

import os
import asyncio
import logging
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import json
import time
from urllib.parse import urlparse

logger = logging.getLogger(__name__)

@dataclass
class EnhancementResult:
    """å†…å®¹å¢å¼ºç»“æœ"""
    success: bool
    enhanced_results: List[Dict[str, Any]]
    original_count: int
    enhanced_count: int
    error_message: str = ""

# Firecrawlé…ç½®ç±» - åŸºäºFiresearchæœ€ä½³å®è·µ
@dataclass
class FirecrawlConfig:
    """Firecrawlé…ç½®å‚æ•°"""
    # é€Ÿç‡é™åˆ¶
    MAX_CONCURRENT_REQUESTS: int = 2        # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
    MAX_SOURCES_TO_SCRAPE: int = 3          # æœ€å¤šæŠ“å–æºæ•°é‡
    REQUEST_DELAY: float = 1.0              # è¯·æ±‚é—´å»¶è¿Ÿ(ç§’)
    
    # é‡è¯•é…ç½®
    MAX_RETRIES: int = 2                    # æœ€å¤§é‡è¯•æ¬¡æ•°
    RETRY_DELAY: float = 2.0                # é‡è¯•å»¶è¿Ÿ(ç§’)
    
    # è¶…æ—¶é…ç½®
    SCRAPE_TIMEOUT: int = 15                # æŠ“å–è¶…æ—¶(ç§’)
    BATCH_TIMEOUT: int = 30                 # æ‰¹é‡è¶…æ—¶(ç§’)
    
    # å†…å®¹é™åˆ¶
    MIN_CONTENT_LENGTH: int = 100           # æœ€å°å†…å®¹é•¿åº¦
    MAX_CONTENT_LENGTH: int = 5000          # æœ€å¤§å†…å®¹é•¿åº¦
    
    # è´¨é‡æ§åˆ¶
    PRIORITY_DOMAINS: List[str] = None      # ä¼˜å…ˆåŸŸå
    EXCLUDED_DOMAINS: List[str] = None      # æ’é™¤åŸŸå
    
    def __post_init__(self):
        if self.PRIORITY_DOMAINS is None:
            self.PRIORITY_DOMAINS = [
                '.edu', '.gov', '.org',           # æƒå¨æœºæ„
                'wikipedia.org', 'arxiv.org',     # å­¦æœ¯èµ„æº
                'github.com', 'stackoverflow.com' # æŠ€æœ¯èµ„æº
            ]
        
        if self.EXCLUDED_DOMAINS is None:
            self.EXCLUDED_DOMAINS = [
                'twitter.com', 'facebook.com', 'instagram.com',  # ç¤¾äº¤åª’ä½“
                'tiktok.com', 'youtube.com',                     # è§†é¢‘å¹³å°
                'pinterest.com', 'reddit.com'                    # å…¶ä»–ç¤¾äº¤
            ]

# å…¨å±€é…ç½®å®ä¾‹
FIRECRAWL_CONFIG = FirecrawlConfig()

class SimpleFirecrawlClient:
    """ç®€åŒ–ç‰ˆFirecrawlå®¢æˆ·ç«¯ - ä¸“ä¸ºV1.5è®¾è®¡"""
    
    def __init__(self):
        self.api_key = os.getenv("FIRECRAWL_API_KEY")
        self.enabled = bool(self.api_key)
        self.client = None
        self.config = FIRECRAWL_CONFIG
        self.last_request_time = 0  # ç”¨äºé€Ÿç‡é™åˆ¶
        
        if self.enabled:
            try:
                # ä¿®å¤ï¼šä½¿ç”¨AsyncFirecrawlAppç”¨äºå¼‚æ­¥æ“ä½œ
                from firecrawl import AsyncFirecrawlApp
                self.client = AsyncFirecrawlApp(api_key=self.api_key)
                logger.info("ğŸ”¥ V1.5 Firecrawlå¼‚æ­¥å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ Firecrawlåˆå§‹åŒ–å¤±è´¥: {e}")
                self.enabled = False
                self.client = None
        else:
            logger.info("â„¹ï¸ Firecrawl API Keyæœªé…ç½®ï¼Œä½¿ç”¨Mockæ¨¡å¼")
    
    def _extract_domain(self, url: str) -> str:
        """æå–åŸŸå"""
        try:
            return urlparse(url).netloc.lower()
        except:
            return ""
    
    def _is_suitable_for_enhancement(self, url: str) -> bool:
        """åˆ¤æ–­URLæ˜¯å¦é€‚åˆå¢å¼º - ä½¿ç”¨é…ç½®åŒ–çš„åŸŸåè¿‡æ»¤"""
        if not url:
            return False
        
        domain = self._extract_domain(url)
        
        # æ£€æŸ¥æ’é™¤åŸŸå
        for excluded in self.config.EXCLUDED_DOMAINS:
            if excluded in domain:
                return False
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹æ’é™¤
        excluded_extensions = ['.pdf', '.doc', '.docx', '.ppt', '.pptx', '.xls', '.xlsx']
        if any(url.lower().endswith(ext) for ext in excluded_extensions):
            return False
        
        return True
    
    def _calculate_priority_score(self, url: str) -> float:
        """è®¡ç®—URLä¼˜å…ˆçº§åˆ†æ•°"""
        domain = self._extract_domain(url)
        score = 0.5  # åŸºç¡€åˆ†æ•°
        
        # ä¼˜å…ˆåŸŸååŠ åˆ†
        for priority in self.config.PRIORITY_DOMAINS:
            if priority in domain:
                score += 0.3
                break
        
        # HTTPSåŠ åˆ†
        if url.startswith('https://'):
            score += 0.1
        
        # é¿å…è¿‡é•¿URL
        if len(url) > 200:
            score -= 0.2
        
        return min(1.0, max(0.0, score))
    
    def _select_priority_urls(self, results: List[Dict], max_count: int = None) -> List[str]:
        """é€‰æ‹©ä¼˜å…ˆå¢å¼ºçš„URLs - ä½¿ç”¨é…ç½®åŒ–çš„é™åˆ¶"""
        if max_count is None:
            max_count = self.config.MAX_SOURCES_TO_SCRAPE
        
        # è®¡ç®—æ¯ä¸ªURLçš„ä¼˜å…ˆçº§åˆ†æ•°
        url_scores = []
        seen_domains = set()
        
        for result in results:
            # å…¼å®¹ä¸åŒçš„URLå­—æ®µåï¼š'url' æˆ– 'link'ï¼ˆGoogleæœç´¢ä½¿ç”¨'link'ï¼‰
            url = result.get('url') or result.get('link', '')
            url = url.strip() if url else ''
            
            if not url or not self._is_suitable_for_enhancement(url):
                continue
            
            # åŸŸåå»é‡ï¼Œæé«˜å¤šæ ·æ€§
            domain = self._extract_domain(url)
            if domain in seen_domains:
                continue
            
            score = self._calculate_priority_score(url)
            url_scores.append((url, score, domain))
            seen_domains.add(domain)
        
        # æŒ‰åˆ†æ•°æ’åºå¹¶é€‰æ‹©å‰Nä¸ª
        url_scores.sort(key=lambda x: x[1], reverse=True)
        selected_urls = [url for url, score, domain in url_scores[:max_count]]
        
        logger.info(f"ğŸ“Œ ä»{len(results)}ä¸ªç»“æœä¸­é€‰æ‹©äº†{len(selected_urls)}ä¸ªä¼˜è´¨URLè¿›è¡Œå¢å¼º")
        return selected_urls
    
    async def _rate_limited_request(self):
        """å®ç°é€Ÿç‡é™åˆ¶"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.config.REQUEST_DELAY:
            sleep_time = self.config.REQUEST_DELAY - time_since_last
            logger.debug(f"â±ï¸ é€Ÿç‡é™åˆ¶ï¼šç­‰å¾… {sleep_time:.1f} ç§’")
            await asyncio.sleep(sleep_time)
        
        self.last_request_time = time.time()
    
    async def _batch_scrape(self, urls: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡æŠ“å–URLs - ä½¿ç”¨ä¸¥æ ¼çš„å¹¶å‘æ§åˆ¶"""
        results = []
        
        # ä½¿ç”¨é…ç½®åŒ–çš„å¹¶å‘é™åˆ¶
        semaphore = asyncio.Semaphore(self.config.MAX_CONCURRENT_REQUESTS)
        
        async def scrape_single_url(url: str) -> Optional[Dict[str, Any]]:
            async with semaphore:
                try:
                    # å®æ–½é€Ÿç‡é™åˆ¶
                    await self._rate_limited_request()
                    
                    logger.info(f"ğŸ”„ æ­£åœ¨æŠ“å–: {self._extract_domain(url)}")
                    
                    # ä½¿ç”¨é…ç½®åŒ–çš„å‚æ•°
                    scrape_kwargs = {
                        "formats": ["markdown"],
                        "only_main_content": True,
                        "timeout": self.config.SCRAPE_TIMEOUT * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                    }
                    
                    # ä¿®å¤ï¼šæ­£ç¡®è°ƒç”¨AsyncFirecrawlApp
                    content = await asyncio.wait_for(
                        self.client.scrape_url(url, **scrape_kwargs),
                        timeout=self.config.SCRAPE_TIMEOUT
                    )
                    
                    # ä¿®å¤ï¼šæ­£ç¡®å¤„ç†AsyncFirecrawlAppçš„å“åº”æ ¼å¼
                    if content:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯ ScrapeResponse å¯¹è±¡
                        if hasattr(content, 'success') and hasattr(content, 'markdown'):
                            if content.success and content.markdown:
                                markdown_text = content.markdown.strip()
                                
                                # å†…å®¹é•¿åº¦æ£€æŸ¥
                                if len(markdown_text) < self.config.MIN_CONTENT_LENGTH:
                                    logger.warning(f"âš ï¸ å†…å®¹è¿‡çŸ­: {self._extract_domain(url)} ({len(markdown_text)} å­—ç¬¦)")
                                    return None
                                
                                # æˆªæ–­è¿‡é•¿å†…å®¹
                                if len(markdown_text) > self.config.MAX_CONTENT_LENGTH:
                                    markdown_text = markdown_text[:self.config.MAX_CONTENT_LENGTH] + "..."
                                    logger.info(f"âœ‚ï¸ å†…å®¹æˆªæ–­: {self._extract_domain(url)} (é™åˆ¶ä¸º {self.config.MAX_CONTENT_LENGTH} å­—ç¬¦)")
                                
                                logger.info(f"âœ… æˆåŠŸæŠ“å–: {self._extract_domain(url)} ({len(markdown_text)} å­—ç¬¦)")
                                
                                # è·å–å…ƒæ•°æ®
                                metadata = getattr(content, 'metadata', {}) or {}
                                title = metadata.get('title', '') if isinstance(metadata, dict) else ''
                                
                                return {
                                    "url": url,
                                    "content": markdown_text,
                                    "title": title or f"Content from {self._extract_domain(url)}",
                                    "word_count": len(markdown_text.split()),
                                    "extraction_time": time.time(),
                                    "metadata": metadata
                                }
                            else:
                                logger.warning(f"âš ï¸ æŠ“å–å¤±è´¥: {self._extract_domain(url)} - success: {getattr(content, 'success', False)}")
                                return None
                        # å…¼å®¹å­—å…¸æ ¼å¼è¿”å›
                        elif isinstance(content, dict) and content.get('markdown'):
                            markdown_text = content['markdown'].strip()
                            
                            # å†…å®¹é•¿åº¦æ£€æŸ¥
                            if len(markdown_text) < self.config.MIN_CONTENT_LENGTH:
                                logger.warning(f"âš ï¸ å†…å®¹è¿‡çŸ­: {self._extract_domain(url)} ({len(markdown_text)} å­—ç¬¦)")
                                return None
                            
                            # æˆªæ–­è¿‡é•¿å†…å®¹
                            if len(markdown_text) > self.config.MAX_CONTENT_LENGTH:
                                markdown_text = markdown_text[:self.config.MAX_CONTENT_LENGTH] + "..."
                                logger.info(f"âœ‚ï¸ å†…å®¹æˆªæ–­: {self._extract_domain(url)} (é™åˆ¶ä¸º {self.config.MAX_CONTENT_LENGTH} å­—ç¬¦)")
                            
                            logger.info(f"âœ… æˆåŠŸæŠ“å–: {self._extract_domain(url)} ({len(markdown_text)} å­—ç¬¦)")
                            
                            return {
                                "url": url,
                                "content": markdown_text,
                                "title": content.get('metadata', {}).get('title', '') or f"Content from {self._extract_domain(url)}",
                                "word_count": len(markdown_text.split()),
                                "extraction_time": time.time(),
                                "metadata": content.get('metadata', {})
                            }
                        else:
                            logger.warning(f"âš ï¸ æ— æœ‰æ•ˆå†…å®¹: {self._extract_domain(url)}")
                            return None
                    else:
                        logger.warning(f"âš ï¸ æ— å“åº”: {self._extract_domain(url)}")
                        return None
                        
                except asyncio.TimeoutError:
                    logger.warning(f"â° æŠ“å–è¶…æ—¶: {self._extract_domain(url)}")
                    return None
                except Exception as e:
                    logger.error(f"âŒ æŠ“å–å¤±è´¥: {self._extract_domain(url)} - {str(e)}")
                    return None
        
        # å¹¶å‘æ‰§è¡ŒæŠ“å–ä»»åŠ¡
        tasks = [scrape_single_url(url) for url in urls]
        
        try:
            completed_results = await asyncio.wait_for(
                asyncio.gather(*tasks, return_exceptions=True),
                timeout=self.config.BATCH_TIMEOUT
            )
            
            # è¿‡æ»¤æœ‰æ•ˆç»“æœ
            for result in completed_results:
                if isinstance(result, dict) and result:
                    results.append(result)
                elif isinstance(result, Exception):
                    logger.error(f"âŒ ä»»åŠ¡å¼‚å¸¸: {result}")
            
        except asyncio.TimeoutError:
            logger.error(f"â° æ‰¹é‡æŠ“å–è¶…æ—¶ ({self.config.BATCH_TIMEOUT}ç§’)")
        
        return results
    
    async def enhance_search_results(self, 
                                   search_results: List[Dict], 
                                   max_enhance: int = 3) -> EnhancementResult:
        """å¢å¼ºæœç´¢ç»“æœ - V1.5æ ¸å¿ƒåŠŸèƒ½"""
        
        if not self.enabled or not self.client:
            logger.info("â„¹ï¸ Firecrawlæœªå¯ç”¨ï¼Œè·³è¿‡å†…å®¹å¢å¼º")
            return EnhancementResult(
                success=False,
                enhanced_results=search_results,
                original_count=len(search_results),
                enhanced_count=0,
                error_message="Firecrawlæœªå¯ç”¨"
            )
        
        logger.info(f"ğŸ” å¼€å§‹å¢å¼º {len(search_results)} æ¡æœç´¢ç»“æœ")
        
        # é€‰æ‹©ä¼˜è´¨URLsè¿›è¡Œå¢å¼º
        priority_urls = self._select_priority_urls(search_results, max_enhance)
        
        if not priority_urls:
            logger.info("â„¹ï¸ æœªæ‰¾åˆ°é€‚åˆå¢å¼ºçš„URL")
            return EnhancementResult(
                success=True,
                enhanced_results=search_results,
                original_count=len(search_results),
                enhanced_count=0,
                error_message="æœªæ‰¾åˆ°é€‚åˆå¢å¼ºçš„URL"
            )
        
        logger.info(f"ğŸ“¥ é€‰æ‹© {len(priority_urls)} ä¸ªURLè¿›è¡Œæ·±åº¦æŠ“å–")
        
        # å¹¶è¡ŒæŠ“å–å†…å®¹
        enhanced_content = await self._batch_scrape(priority_urls)
        
        if not enhanced_content:
            logger.warning("âš ï¸ æ‰€æœ‰URLå¢å¼ºéƒ½å¤±è´¥äº†")
            return EnhancementResult(
                success=False,
                enhanced_results=search_results,
                original_count=len(search_results),
                enhanced_count=0,
                error_message="æ‰€æœ‰URLå¢å¼ºéƒ½å¤±è´¥"
            )
        
        # åˆå¹¶å¢å¼ºå†…å®¹åˆ°åŸå§‹ç»“æœ
        enhanced_results = self._merge_enhanced_content(
            search_results, enhanced_content
        )
        
        logger.info(f"âœ… æˆåŠŸå¢å¼º {len(enhanced_content)} æ¡å†…å®¹")
        
        return EnhancementResult(
            success=True,
            enhanced_results=enhanced_results,
            original_count=len(search_results),
            enhanced_count=len(enhanced_content)
        )
    
    def _merge_enhanced_content(self, 
                              original_results: List[Dict], 
                              enhanced_content: List[Dict]) -> List[Dict]:
        """åˆå¹¶å¢å¼ºå†…å®¹åˆ°åŸå§‹ç»“æœ"""
        enhanced_urls = {item['url'] for item in enhanced_content}
        
        merged_results = []
        
        # æ·»åŠ å¢å¼ºåçš„ç»“æœ
        for result in original_results:
            result_copy = result.copy()  # é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
            
            # å…¼å®¹ä¸åŒçš„URLå­—æ®µå
            result_url = result.get('url') or result.get('link', '')
            
            if result_url in enhanced_urls:
                # æ‰¾åˆ°å¯¹åº”çš„å¢å¼ºå†…å®¹
                enhanced = next(
                    (item for item in enhanced_content if item['url'] == result_url), 
                    None
                )
                if enhanced:
                    # åˆå¹¶å†…å®¹
                    original_content = result_copy.get('content', result_copy.get('snippet', ''))
                    enhanced_text = enhanced['content']
                    
                    # æ„å»ºå¢å¼ºåçš„å†…å®¹
                    if original_content:
                        result_copy['content'] = f"{original_content}\n\n**ğŸ”¥ æ·±åº¦å†…å®¹åˆ†æ:**\n{enhanced_text}"
                    else:
                        result_copy['content'] = f"**ğŸ”¥ æ·±åº¦å†…å®¹åˆ†æ:**\n{enhanced_text}"
                    
                    # ç»Ÿä¸€URLå­—æ®µï¼ˆå¦‚æœåŸæ¥æ˜¯'link'ï¼Œä¹Ÿæ·»åŠ 'url'å­—æ®µï¼‰
                    if not result_copy.get('url') and result_copy.get('link'):
                        result_copy['url'] = result_copy['link']
                    
                    # æ·»åŠ å¢å¼ºæ ‡è®°
                    result_copy['enhanced'] = True
                    result_copy['enhancement_source'] = 'firecrawl'
                    result_copy['original_length'] = len(original_content)
                    result_copy['enhanced_length'] = len(enhanced_text)
                    
                    logger.info(f"ğŸ”— å·²å¢å¼º: {enhanced.get('domain', 'unknown')} (+{len(enhanced_text)} å­—ç¬¦)")
            
            merged_results.append(result_copy)
        
        return merged_results

# åŒæ­¥åŒ…è£…å™¨ï¼Œé€‚é…V1çš„åŒæ­¥è°ƒç”¨æ¨¡å¼
def enhance_search_results_sync(search_results: List[Dict], max_enhance: int = 3) -> EnhancementResult:
    """åŒæ­¥ç‰ˆæœ¬çš„æœç´¢ç»“æœå¢å¼º - é€‚é…V1çš„åŒæ­¥è°ƒç”¨"""
    try:
        # æ£€æŸ¥æ˜¯å¦åœ¨å¼‚æ­¥ç¯å¢ƒä¸­
        try:
            loop = asyncio.get_running_loop()
            # å¦‚æœå·²æœ‰äº‹ä»¶å¾ªç¯ï¼Œä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œ
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(
                    asyncio.run, 
                    firecrawl_client.enhance_search_results(search_results, max_enhance)
                )
                return future.result(timeout=45)  # 45ç§’æ€»è¶…æ—¶
        except RuntimeError:
            # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œç›´æ¥è¿è¡Œ
            return asyncio.run(
                firecrawl_client.enhance_search_results(search_results, max_enhance)
            )
    except Exception as e:
        logger.error(f"âŒ åŒæ­¥å¢å¼ºå¤±è´¥: {e}")
        return EnhancementResult(
            success=False,
            enhanced_results=search_results,
            original_count=len(search_results),
            enhanced_count=0,
            error_message=str(e)
        )

# å…¨å±€å®ä¾‹
firecrawl_client = SimpleFirecrawlClient()

# æµ‹è¯•å‡½æ•°
def test_firecrawl_integration():
    """æµ‹è¯•Firecrawlé›†æˆåŠŸèƒ½"""
    from dotenv import load_dotenv
    load_dotenv()  # ç¡®ä¿åŠ è½½.envæ–‡ä»¶
    
    print("ğŸ§ª æµ‹è¯•V1.5 Firecrawlé›†æˆ...")
    
    # é‡æ–°åˆå§‹åŒ–å®¢æˆ·ç«¯ä»¥ä½¿ç”¨æ–°çš„ç¯å¢ƒå˜é‡
    global firecrawl_client
    firecrawl_client = SimpleFirecrawlClient()
    
    # æ¨¡æ‹Ÿæœç´¢ç»“æœ
    test_results = [
        {
            'title': 'Test Article',
            'url': 'https://example.com/article',
            'content': 'Original content...'
        }
    ]
    
    result = enhance_search_results_sync(test_results, max_enhance=1)
    
    print(f"æµ‹è¯•ç»“æœ:")
    print(f"- æˆåŠŸ: {result.success}")
    print(f"- åŸå§‹æ•°é‡: {result.original_count}")
    print(f"- å¢å¼ºæ•°é‡: {result.enhanced_count}")
    print(f"- é”™è¯¯ä¿¡æ¯: {result.error_message}")
    
    return result

if __name__ == "__main__":
    test_firecrawl_integration() 