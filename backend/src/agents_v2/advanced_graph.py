"""
é«˜çº§LangGraphå›¾ç»“æ„
è¿æ¥æ™ºèƒ½è§„åˆ’å™¨ã€ä»»åŠ¡åè°ƒå™¨ã€å†…å®¹å¢å¼ºå™¨ç­‰æ ¸å¿ƒèŠ‚ç‚¹
"""

from langgraph.graph import StateGraph, START, END
from langchain_core.runnables import RunnableConfig

from .advanced_state import AdvancedResearchState
from .planner import planner_node
from .coordinator import task_coordinator_node, decide_next_step_in_plan
from .enhancer import content_enhancement_node, should_enhance_content
from .api_utils import retry_manager, robust_web_search, enhance_content_with_firecrawl

# é‡ç”¨ç°æœ‰çš„èŠ‚ç‚¹ï¼ˆå…¼å®¹æ€§ï¼‰
from agent.graph import (
    generate_queries_node,
    web_search_node, 
    reflect_node,
    generate_report_node
)


def create_advanced_research_graph():
    """åˆ›å»ºé«˜çº§ç ”ç©¶å›¾"""
    
    # åˆ›å»ºçŠ¶æ€å›¾
    builder = StateGraph(AdvancedResearchState)
    
    # ===== æ·»åŠ æ‰€æœ‰èŠ‚ç‚¹ =====
    
    # æ–°çš„é«˜çº§èŠ‚ç‚¹
    builder.add_node("planner", planner_node)
    builder.add_node("task_coordinator", task_coordinator_node)
    builder.add_node("content_enhancer", content_enhancement_node)
    
    # é‡ç”¨çš„ç°æœ‰èŠ‚ç‚¹ï¼ˆé€‚é…åï¼‰
    builder.add_node("generate_query", generate_query_adapter)
    builder.add_node("web_research", web_research_adapter) 
    builder.add_node("reflection", reflection_adapter)
    builder.add_node("finalize_answer", finalize_answer_adapter)
    
    # ===== å®šä¹‰å›¾ç»“æ„ =====
    
    # 1. å…¥å£ï¼šæ™ºèƒ½è§„åˆ’
    builder.add_edge(START, "planner")
    
    # 2. è§„åˆ’ -> ä»»åŠ¡åè°ƒ
    builder.add_edge("planner", "task_coordinator")
    
    # 3. ä»»åŠ¡åè°ƒ -> æŸ¥è¯¢ç”Ÿæˆï¼ˆæ¡ä»¶è¾¹ï¼‰
    builder.add_conditional_edges(
        "task_coordinator",
        route_from_coordinator,
        {
            "continue_task": "generate_query",
            "start_new_task": "generate_query", 
            "finalize_report": "finalize_answer"
        }
    )
    
    # 4. æŸ¥è¯¢ç”Ÿæˆ -> ç½‘ç»œæœç´¢ï¼ˆå¹¶è¡Œï¼‰
    builder.add_conditional_edges(
        "generate_query", 
        continue_to_web_research_advanced, 
        ["web_research"]
    )
    
    # 5. ç½‘ç»œæœç´¢ -> åæ€
    builder.add_edge("web_research", "reflection")
    
    # 6. åæ€ -> å†…å®¹å¢å¼ºï¼ˆæ¡ä»¶è¾¹ï¼‰
    builder.add_conditional_edges(
        "reflection",
        should_enhance_content_advanced,
        {
            "enhance": "content_enhancer",
            "skip_enhancement": "task_coordinator"
        }
    )
    
    # 7. å†…å®¹å¢å¼º -> ä»»åŠ¡åè°ƒ
    builder.add_edge("content_enhancer", "task_coordinator")
    
    # 8. æœ€ç»ˆç­”æ¡ˆ -> ç»“æŸ
    builder.add_edge("finalize_answer", END)
    
    # ç¼–è¯‘å›¾
    graph = builder.compile()
    
    print("ğŸ¯ é«˜çº§ç ”ç©¶å›¾åˆ›å»ºå®Œæˆ")
    print("  èŠ‚ç‚¹æ•°é‡ï¼š", len(builder.nodes))
    
    return graph


# ===== è·¯ç”±å‡½æ•° =====

def route_from_coordinator(state: AdvancedResearchState) -> str:
    """ä»ä»»åŠ¡åè°ƒå™¨è·¯ç”±åˆ°ä¸‹ä¸€æ­¥"""
    
    next_step = state.get("next_step", "continue_task")
    is_complete = state.get("is_complete", False)
    
    print(f"ğŸ›ï¸ åè°ƒå™¨è·¯ç”±è°ƒè¯•:")
    print(f"  next_step: {next_step}")
    print(f"  is_complete: {is_complete}")
    print(f"  çŠ¶æ€å­—æ®µ: {list(state.keys())}")
    print(f"ğŸ›ï¸ åè°ƒå™¨è·¯ç”±ï¼š{next_step}")
    
    # å¦‚æœç ”ç©¶å·²å®Œæˆï¼Œå¼ºåˆ¶è·¯ç”±åˆ°æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆ
    if is_complete:
        print(f"ğŸ æ£€æµ‹åˆ°ç ”ç©¶å®Œæˆï¼Œå¼ºåˆ¶è·¯ç”±åˆ°æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆ")
        return "finalize_report"
    
    if next_step == "finalize_report":
        print(f"ğŸ è·¯ç”±åˆ°æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆ")
        return "finalize_report"  # è¿™ä¸ªä¼šæ˜ å°„åˆ° finalize_answer èŠ‚ç‚¹
    elif next_step == "start_new_task":
        print(f"ğŸ”„ è·¯ç”±åˆ°æ–°ä»»åŠ¡å¼€å§‹")
        return "start_new_task"
    else:
        print(f"ğŸ”„ è·¯ç”±åˆ°ç»§ç»­ä»»åŠ¡")
        return "continue_task"


def should_enhance_content_advanced(state: AdvancedResearchState) -> str:
    """å†³å®šæ˜¯å¦è¿›è¡Œå†…å®¹å¢å¼º"""
    
    print(f"ğŸ”€ å†…å®¹å¢å¼ºå†³ç­–å¼€å§‹...")
    print(f"ğŸ”€ is_completeçŠ¶æ€: {state.get('is_complete', False)}")
    print(f"ğŸ”€ çŠ¶æ€å­—æ®µæ•°é‡: {len(state.keys())}")
    print(f"ğŸ”€ ç ”ç©¶è®¡åˆ’é•¿åº¦: {len(state.get('research_plan', []))}")
    
    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»å®Œæˆç ”ç©¶
    if state.get("is_complete", False):
        print("ğŸ ç ”ç©¶å·²å®Œæˆï¼Œè·³è¿‡å¢å¼ºï¼Œå›åˆ°ä»»åŠ¡åè°ƒå™¨")
        print("ğŸ”€ è¿”å›è·¯ç”±: skip_enhancement -> task_coordinator")
        print("ğŸ”€ğŸ”€ğŸ”€ å³å°†è¿”å› 'skip_enhancement' è·¯ç”±")
        return "skip_enhancement"
    
    # ç®€åŒ–ç‰ˆå†³ç­–é€»è¾‘
    current_findings = state.get("web_research_results", [])
    
    if len(current_findings) < 3:
        print("ğŸ” å†…å®¹è¾ƒå°‘ï¼Œå¯ç”¨å¢å¼º")
        print("ğŸ”€ è¿”å›è·¯ç”±: enhance -> content_enhancer")
        print("ğŸ”€ğŸ”€ğŸ”€ å³å°†è¿”å› 'enhance' è·¯ç”±")
        return "enhance"
    else:
        print("â­ï¸ å†…å®¹å……è¶³ï¼Œè·³è¿‡å¢å¼º")
        print("ğŸ”€ è¿”å›è·¯ç”±: skip_enhancement -> task_coordinator")
        print("ğŸ”€ğŸ”€ğŸ”€ å³å°†è¿”å› 'skip_enhancement' è·¯ç”±")
        return "skip_enhancement"


def continue_to_web_research_advanced(state: AdvancedResearchState):
    """å‘é€æœç´¢æŸ¥è¯¢åˆ°ç½‘ç»œç ”ç©¶èŠ‚ç‚¹ï¼ˆå¹¶è¡Œï¼‰"""
    
    from langgraph.types import Send
    
    # è°ƒè¯•ï¼šæ‰“å°æ‰€æœ‰å¯èƒ½çš„æŸ¥è¯¢å­—æ®µ
    print(f"ğŸ” è°ƒè¯•æŸ¥è¯¢å­—æ®µ:")
    print(f"  query_list: {state.get('query_list', 'NOT_FOUND')}")
    print(f"  search_queries: {state.get('search_queries', 'NOT_FOUND')}")
    print(f"  executed_search_queries: {state.get('executed_search_queries', 'NOT_FOUND')}")
    print(f"  æ‰€æœ‰çŠ¶æ€å­—æ®µ: {list(state.keys())}")
    
    # è·å–ç”Ÿæˆçš„æŸ¥è¯¢ - å°è¯•å¤šä¸ªå¯èƒ½çš„å­—æ®µå
    query_list = (
        state.get("query_list") or 
        state.get("search_queries") or 
        state.get("executed_search_queries") or
        []
    )
    
    print(f"ğŸ” æœ€ç»ˆè·å–çš„æŸ¥è¯¢åˆ—è¡¨: {query_list}")
    
    if not query_list:
        print("âš ï¸ æ²¡æœ‰ç”Ÿæˆæœç´¢æŸ¥è¯¢")
        return []
    
    # è·å–å½“å‰ä»»åŠ¡ä¿¡æ¯
    research_plan = state.get("research_plan", [])
    current_pointer = state.get("current_task_pointer", 0)
    current_task_id = "unknown"
    
    if current_pointer < len(research_plan):
        task_data = research_plan[current_pointer]
        if isinstance(task_data, dict):
            current_task_id = task_data.get("id", "unknown")
        else:
            current_task_id = getattr(task_data, 'id', 'unknown')
    
    print(f"ğŸ”„ å¹¶è¡Œå‘é€ {len(query_list)} ä¸ªæœç´¢æŸ¥è¯¢")
    print(f"ğŸ”„ å½“å‰ä»»åŠ¡ID: {current_task_id}")
    
    # åˆ›å»ºå¹¶è¡Œæœç´¢ä»»åŠ¡
    return [
        Send("web_research", {
            "search_query": query,
            "id": idx,
            "current_task_id": current_task_id
        })
        for idx, query in enumerate(query_list)
    ]


# ===== é€‚é…å™¨å‡½æ•° =====

def generate_query_adapter(state: AdvancedResearchState, config: RunnableConfig):
    """æŸ¥è¯¢ç”Ÿæˆé€‚é…å™¨"""
    
    print(f"ğŸ”§ æŸ¥è¯¢ç”Ÿæˆé€‚é…å™¨å¯åŠ¨...")
    print(f"ğŸ”§ è¾“å…¥çŠ¶æ€å­—æ®µ: {list(state.keys())}")
    
    # è½¬æ¢çŠ¶æ€æ ¼å¼ä»¥å…¼å®¹ç°æœ‰èŠ‚ç‚¹
    adapted_state = {
        "user_query": state.get("user_query", ""),
        "plan": state.get("plan", []),
        "current_task_pointer": state.get("current_task_pointer", 0),
        "messages": state.get("messages", []),
        "cycle_count": state.get("cycle_count", 0),
        "critique": state.get("critique", "")
    }
    
    print(f"ğŸ”§ è°ƒç”¨V1 generate_queries_node...")
    # è°ƒç”¨ç°æœ‰å‡½æ•°
    result = generate_queries_node(adapted_state)
    print(f"ğŸ”§ V1è¿”å›å­—æ®µ: {list(result.keys())}")
    print(f"ğŸ”§ V1è¿”å›çš„search_queries: {result.get('search_queries', 'NOT_FOUND')}")
    
    # æ£€æŸ¥æ˜¯å¦æŸ¥è¯¢ç”Ÿæˆå¤±è´¥
    search_queries = result.get("search_queries", [])
    if not search_queries:
        # å¦‚æœæŸ¥è¯¢ç”Ÿæˆå¤±è´¥ï¼Œæä¾›fallbackæŸ¥è¯¢
        current_task = get_current_task(state)
        fallback_queries = [
            f"{current_task.description} æœ€æ–°ç ”ç©¶",
            f"{current_task.description} æ¡ˆä¾‹åˆ†æ",
            f"{current_task.description} æŠ€æœ¯è¶‹åŠ¿"
        ]
        print(f"ğŸ”§ æŸ¥è¯¢ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨fallbackæŸ¥è¯¢: {fallback_queries}")
        search_queries = fallback_queries
    
    # æ›´æ–°çŠ¶æ€
    updated_state = {**state}
    updated_state.update(result)
    updated_state["query_list"] = search_queries  # V2éœ€è¦çš„å­—æ®µå
    
    print(f"ğŸ”§ é€‚é…å™¨è¿”å›å­—æ®µ: {list(updated_state.keys())}")
    print(f"ğŸ”§ é€‚é…å™¨è¿”å›çš„æŸ¥è¯¢: {search_queries}")
    
    return updated_state


async def web_research_adapter(state: AdvancedResearchState, config: RunnableConfig):
    """ç½‘ç»œæœç´¢é€‚é…å™¨ - æ”¯æŒå¹¶è¡Œæœç´¢å’Œå†…å®¹å¢å¼º"""
    
    print(f"ğŸ” V2å¹¶è¡Œç½‘ç»œæœç´¢é€‚é…å™¨å¯åŠ¨...")
    print(f"ğŸ” è¾“å…¥çŠ¶æ€å­—æ®µ: {list(state.keys())}")
    print(f"ğŸ” search_query: {state.get('search_query', 'NOT_FOUND')}")
    print(f"ğŸ” id: {state.get('id', 'NOT_FOUND')}")
    print(f"ğŸ” current_task_id: {state.get('current_task_id', 'NOT_FOUND')}")
    
    # è·å–æœç´¢æŸ¥è¯¢
    search_query = state.get("search_query", "")
    if not search_query:
        print("âš ï¸ æ²¡æœ‰æœç´¢æŸ¥è¯¢ï¼Œè¿”å›ç©ºç»“æœ")
        return {
            "web_research_results": [],
            "sources_gathered": [],
            "search_results": [],
            "current_task_detailed_findings": []
        }
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨å¹¶è¡Œæœç´¢
    enable_parallel = state.get("enable_parallel_search", True)
    enable_enhancement = state.get("enable_content_enhancement", True)
    
    if enable_parallel:
        print(f"ğŸš€ å¯ç”¨å¹¶è¡Œæœç´¢æ¨¡å¼")
        
        # ç”Ÿæˆç›¸å…³æŸ¥è¯¢è¿›è¡Œå¹¶è¡Œæœç´¢
        related_queries = [
            search_query,
            f"{search_query} æ¡ˆä¾‹ç ”ç©¶",
            f"{search_query} æœ€æ–°å‘å±•",
        ]
        
        # ä½¿ç”¨å¹¶è¡Œæœç´¢å·¥å…·
        try:
            # åˆ›å»ºæœç´¢å‡½æ•°çš„åŒ…è£…å™¨
            async def single_search(query):
                adapted_state = {
                    "search_query": query,
                    "search_queries": [query],
                    "id": state.get("id", 0),
                    "current_task_id": state.get("current_task_id", "unknown"),
                    "cycle_count": state.get("cycle_count", 1),
                    "user_query": state.get("user_query", "")
                }
                return web_search_node(adapted_state)
            
            # å¹¶è¡Œæ‰§è¡Œæœç´¢
            all_results = await robust_web_search(single_search, related_queries)
            
            # åˆå¹¶æ‰€æœ‰ç»“æœ
            web_results = []
            sources = []
            
            for result_data in all_results:
                if isinstance(result_data, dict):
                    web_results.extend(result_data.get("search_results", []))
                    sources.extend(result_data.get("sources_gathered", []))
            
            print(f"ğŸš€ å¹¶è¡Œæœç´¢å®Œæˆ: {len(web_results)} æ¡ç»“æœ")
            
        except Exception as e:
            print(f"âŒ å¹¶è¡Œæœç´¢å¤±è´¥ï¼Œå›é€€åˆ°å•ä¸€æœç´¢: {e}")
            # å›é€€åˆ°å•ä¸€æœç´¢
            adapted_state = {
                "search_query": search_query,
                "search_queries": [search_query],
                "id": state.get("id", 0),
                "current_task_id": state.get("current_task_id", "unknown"),
                "cycle_count": state.get("cycle_count", 1),
                "user_query": state.get("user_query", "")
            }
            result = web_search_node(adapted_state)
            web_results = result.get("search_results", [])
            sources = result.get("sources_gathered", [])
    
    else:
        print(f"ğŸ” ä½¿ç”¨æ ‡å‡†æœç´¢æ¨¡å¼")
        # æ ‡å‡†å•ä¸€æœç´¢
        adapted_state = {
            "search_query": search_query,
            "search_queries": [search_query],
            "id": state.get("id", 0),
            "current_task_id": state.get("current_task_id", "unknown"),
            "cycle_count": state.get("cycle_count", 1),
            "user_query": state.get("user_query", "")
        }
        
        result = web_search_node(adapted_state)
        web_results = result.get("search_results", [])
        sources = result.get("sources_gathered", [])
    
    # å†…å®¹å¢å¼ºå¤„ç†
    enhanced_results = web_results
    if enable_enhancement and web_results:
        print(f"ğŸ”¥ å¯ç”¨Firecrawlå†…å®¹å¢å¼º")
        try:
            # è½¬æ¢æœç´¢ç»“æœæ ¼å¼ä»¥é€‚é…å¢å¼ºå‡½æ•°
            formatted_results = []
            for result in web_results:
                if isinstance(result, dict):
                    formatted_results.append(result)
                else:
                    # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œåˆ›å»ºåŸºæœ¬æ ¼å¼
                    formatted_results.append({
                        "title": "æœç´¢ç»“æœ",
                        "snippet": str(result),
                        "url": f"https://example.com/search/{len(formatted_results)}"
                    })
            
            enhanced_results = await enhance_content_with_firecrawl(
                formatted_results,
                {
                    "max_urls": 3,
                    "quality_threshold": 0.6
                }
            )
            
            print(f"ğŸ”¥ å†…å®¹å¢å¼ºå®Œæˆ: {len(enhanced_results)} æ¡ç»“æœ")
            
        except Exception as e:
            print(f"âŒ å†…å®¹å¢å¼ºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ç»“æœ: {e}")
            enhanced_results = web_results
    
    # æ„å»ºè¿”å›ç»“æœ
    v2_result = {
        "web_research_results": enhanced_results,
        "sources_gathered": sources,
        "search_results": enhanced_results,  # v1å…¼å®¹
        "parallel_search_results": enhanced_results,  # æ ‡è®°ä¸ºå¹¶è¡Œæœç´¢ç»“æœ
        
        # ä»»åŠ¡ç‰¹å®šç»“æœ
        "current_task_detailed_findings": [{
            "task_id": state.get("current_task_id", "unknown"),
            "content": content,
            "source": "parallel_web_search" if enable_parallel else "web_search",
            "enhanced": enable_enhancement,
            "timestamp": "now"
        } for content in enhanced_results] if enhanced_results else []
    }
    
    print(f"ğŸ” V2ç½‘ç»œæœç´¢é€‚é…å™¨è¿”å›å­—æ®µ: {list(v2_result.keys())}")
    print(f"ğŸ” è¿”å›çš„æœç´¢ç»“æœæ•°é‡: {len(v2_result.get('search_results', []))}")
    print(f"ğŸ” å¹¶è¡Œæœç´¢: {enable_parallel}, å†…å®¹å¢å¼º: {enable_enhancement}")
    
    return v2_result


def reflection_adapter(state: AdvancedResearchState, config: RunnableConfig):
    """åæ€é€‚é…å™¨"""
    
    print(f"ğŸ¤” åæ€é€‚é…å™¨å¯åŠ¨...")
    print(f"ğŸ¤” è¾“å…¥çŠ¶æ€å­—æ®µ: {list(state.keys())}")
    
    # è½¬æ¢çŠ¶æ€æ ¼å¼
    adapted_state = {
        "user_query": state.get("user_query", ""),  # ç¡®ä¿åŒ…å«user_query
        "web_research_result": state.get("web_research_results", []),
        "search_results": state.get("search_results", []),  # V1å…¼å®¹å­—æ®µ
        "sources_gathered": state.get("sources_gathered", []),
        "plan": state.get("plan", []),
        "current_task_pointer": state.get("current_task_pointer", 0),
        "research_loop_count": state.get("current_task_loop_count", 0),
        "cycle_count": state.get("cycle_count", 1),  # V1éœ€è¦çš„å­—æ®µ
        "number_of_ran_queries": len(state.get("executed_search_queries", [])),
        "scenario_type": state.get("scenario_type", "default")
    }
    
    print(f"ğŸ¤” ä¼ é€’ç»™V1çš„å­—æ®µ: {list(adapted_state.keys())}")
    print(f"ğŸ¤” user_query: {adapted_state.get('user_query', 'NOT_FOUND')}")
    print(f"ğŸ¤” æœç´¢ç»“æœæ•°é‡: {len(adapted_state.get('search_results', []))}")
    
    # è°ƒç”¨ç°æœ‰å‡½æ•°
    result = reflect_node(adapted_state)
    print(f"ğŸ¤” V1è¿”å›å­—æ®µ: {list(result.keys())}")
    
    # è½¬æ¢ç»“æœæ ¼å¼
    v2_result = {
        # ä¿æŒåŸæœ‰çŠ¶æ€
        **state,
        
        # æ˜ å°„V1è¿”å›çš„å­—æ®µ
        "reflection_is_sufficient": result.get("reflection_is_sufficient", False),
        "reflection_knowledge_gap": result.get("reflection_knowledge_gap", ""),
        "reflection_follow_up_queries": result.get("reflection_follow_up_queries", []),
        "critique": result.get("critique", ""),  # v1å…¼å®¹
        "is_complete": result.get("is_complete", False),
        "cycle_count": result.get("cycle_count", state.get("cycle_count", 1)),
        
        # æ›´æ–°ä»»åŠ¡è½®æ¬¡
        "current_task_loop_count": state.get("current_task_loop_count", 0) + 1
    }
    
    print(f"ğŸ¤” åæ€é€‚é…å™¨è¿”å›å­—æ®µ: {list(v2_result.keys())}")
    print(f"ğŸ¤” is_complete: {v2_result.get('is_complete', 'NOT_FOUND')}")
    
    return v2_result


def finalize_answer_adapter(state: AdvancedResearchState, config: RunnableConfig):
    """æœ€ç»ˆç­”æ¡ˆé€‚é…å™¨"""
    
    print(f"ğŸ“ğŸ“ğŸ“ æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆé€‚é…å™¨å¯åŠ¨!!! ğŸ“ğŸ“ğŸ“")
    print(f"ğŸ“ è¾“å…¥çŠ¶æ€å­—æ®µ: {list(state.keys())}")
    print(f"ğŸ“ ç”¨æˆ·æŸ¥è¯¢: {state.get('user_query', 'NOT_FOUND')}")
    
    # å®‰å…¨è·å–åˆ—è¡¨é•¿åº¦
    task_results = state.get('task_results', []) or []
    global_memory = state.get('global_memory', []) or []
    research_plan = state.get('research_plan', []) or []
    
    print(f"ğŸ“ ä»»åŠ¡ç»“æœæ•°é‡: {len(task_results)}")
    print(f"ğŸ“ å…¨å±€è®°å¿†æ•°é‡: {len(global_memory)}")
    print(f"ğŸ“ ç ”ç©¶è®¡åˆ’æ•°é‡: {len(research_plan)}")
    
    # æ”¶é›†æ‰€æœ‰æœç´¢ç»“æœ
    all_search_results = []
    
    # ä»web_research_resultsæ”¶é›†
    web_results = state.get("web_research_results", []) or []
    if web_results:
        all_search_results.extend(web_results)
        print(f"ğŸ“ ä»web_research_resultsæ”¶é›†åˆ° {len(web_results)} æ¡ç»“æœ")
    
    # ä»search_resultsæ”¶é›†ï¼ˆV1å…¼å®¹å­—æ®µï¼‰
    search_results = state.get("search_results", []) or []
    if search_results:
        all_search_results.extend(search_results)
        print(f"ğŸ“ ä»search_resultsæ”¶é›†åˆ° {len(search_results)} æ¡ç»“æœ")
    
    # ä»parallel_search_resultsæ”¶é›†
    parallel_results = state.get("parallel_search_results", []) or []
    if parallel_results:
        all_search_results.extend(parallel_results)
        print(f"ğŸ“ ä»parallel_search_resultsæ”¶é›†åˆ° {len(parallel_results)} æ¡ç»“æœ")
    
    # ä»current_task_detailed_findingsæ”¶é›†
    detailed_findings = state.get("current_task_detailed_findings", []) or []
    if detailed_findings:
        # è½¬æ¢æ ¼å¼
        for finding in detailed_findings:
            if isinstance(finding, dict) and "content" in finding:
                all_search_results.append(finding["content"])
        print(f"ğŸ“ ä»current_task_detailed_findingsæ”¶é›†åˆ° {len(detailed_findings)} æ¡ç»“æœ")
    
    print(f"ğŸ“ æ€»å…±æ”¶é›†åˆ° {len(all_search_results)} æ¡æœç´¢ç»“æœ")
    
    # å¦‚æœæ²¡æœ‰æœç´¢ç»“æœï¼Œåˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„ç»“æœ
    if not all_search_results:
        all_search_results = [f"åŸºäºæŸ¥è¯¢'{state.get('user_query', '')}' çš„ç ”ç©¶ç»“æœ"]
        print(f"ğŸ“ æ²¡æœ‰æ‰¾åˆ°æœç´¢ç»“æœï¼Œåˆ›å»ºé»˜è®¤ç»“æœ")
    
    # è½¬æ¢çŠ¶æ€æ ¼å¼ä»¥å…¼å®¹V1 generate_report_node
    adapted_state = {
        "user_query": state.get("user_query", ""),
        "search_results": all_search_results,  # V1å¿…éœ€å­—æ®µ
        "sources_gathered": state.get("sources_gathered", []),
        "ledger": [],  # ä»task_resultsæ„å»º
        "final_report_markdown": state.get("final_report_markdown", ""),
        "cycle_count": state.get("cycle_count", 1),
        "scenario_type": state.get("scenario_type", "default")
    }
    
    print(f"ğŸ“ å¼€å§‹æ„å»ºledger...")
    # æ„å»ºledgeræ ¼å¼
    task_results_list = state.get("task_results", []) or []
    for task_result in task_results_list:
        if hasattr(task_result, 'task_id'):
            ledger_entry = {
                "task_id": task_result.task_id,
                "description": task_result.description,
                "findings_summary": task_result.findings_summary,
                "detailed_snippets": task_result.detailed_findings,
                "citations_for_snippets": task_result.sources_citations
            }
            adapted_state["ledger"].append(ledger_entry)
            print(f"ğŸ“ æ·»åŠ ledgeræ¡ç›®: {task_result.task_id}")
    
    print(f"ğŸ“ ledgeræ„å»ºå®Œæˆï¼Œå…± {len(adapted_state['ledger'])} ä¸ªæ¡ç›®")
    print(f"ğŸ“ é€‚é…çŠ¶æ€å­—æ®µ: {list(adapted_state.keys())}")
    print(f"ğŸ“ search_resultsæ•°é‡: {len(adapted_state['search_results'])}")
    print(f"ğŸ“ è°ƒç”¨V1 generate_report_node...")
    
    # ç›´æ¥è°ƒç”¨V1æŠ¥å‘Šç”Ÿæˆï¼ˆåŒæ­¥æ–¹å¼ï¼‰
    print(f"ğŸ“ è°ƒç”¨V1æŠ¥å‘Šç”Ÿæˆ...")
    try:
        result = generate_report_node(adapted_state)
        print(f"ğŸ“ V1æŠ¥å‘Šç”ŸæˆæˆåŠŸ")
    except Exception as e:
        print(f"ğŸ“ æŠ¥å‘Šç”Ÿæˆå¼‚å¸¸: {e}")
        result = {
            "report": f"æŠ¥å‘Šç”Ÿæˆé‡åˆ°å¼‚å¸¸: {str(e)}",
            "final_report_markdown": f"# æŠ¥å‘Šç”Ÿæˆå¼‚å¸¸\n\né”™è¯¯ä¿¡æ¯: {str(e)}",
            "is_complete": True
        }
    
    print(f"ğŸ“ V1è¿”å›å­—æ®µ: {list(result.keys())}")
    
    # å®‰å…¨è·å–æŠ¥å‘Šå†…å®¹ - ä¼˜å…ˆä»reportå­—æ®µè·å–
    final_report = result.get('report') or result.get('final_report_markdown') or ""
    print(f"ğŸ“ æŠ¥å‘Šé•¿åº¦: {len(final_report)}")
    print(f"ğŸ“ V1è¿”å›çš„reportå­—æ®µ: {len(result.get('report') or '')}")
    print(f"ğŸ“ V1è¿”å›çš„final_report_markdownå­—æ®µ: {len(result.get('final_report_markdown') or '')}")
    
    # è½¬æ¢ç»“æœæ ¼å¼
    final_result = {
        "final_report_markdown": final_report,
        "report": final_report,  # v1å…¼å®¹
        "is_complete": True,
        "execution_summary": {
            "total_tasks": len(research_plan),
            "completed_tasks": len(state.get("completed_tasks", []) or []),
            "total_cycles": state.get("total_research_loops", 0),
            "api_calls": 0,  # æš‚æ—¶è®¾ä¸º0ï¼Œé¿å…ä¾èµ–retry_manager
            "success_rate": 1.0
        }
    }
    
    print(f"ğŸ“ æœ€ç»ˆæŠ¥å‘Šé€‚é…å™¨è¿”å›å­—æ®µ: {list(final_result.keys())}")
    print(f"ğŸ“ æœ€ç»ˆæŠ¥å‘Šé•¿åº¦: {len(final_result.get('final_report_markdown', ''))}")
    
    return final_result


# ===== å›¾å®ä¾‹åŒ– =====

def get_advanced_research_graph():
    """è·å–é«˜çº§ç ”ç©¶å›¾å®ä¾‹"""
    return create_advanced_research_graph() 