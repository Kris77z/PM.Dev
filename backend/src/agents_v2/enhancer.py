"""
å†…å®¹å¢å¼ºå™¨èŠ‚ç‚¹
æ™ºèƒ½å†³ç­–æ·±åº¦å†…å®¹æŠ“å–å’Œè´¨é‡è¯„ä¼°
"""

import os
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from langchain_core.runnables import RunnableConfig
from pydantic import BaseModel, Field
from langchain_google_genai import ChatGoogleGenerativeAI

from .advanced_state import AdvancedResearchState, ContentQualityAssessment
from .planner import get_current_task


@dataclass
class EnhancementDecision:
    """å¢å¼ºå†³ç­–ç»“æœ"""
    needs_enhancement: bool
    confidence_score: float  # 0-1
    enhancement_type: str    # "depth", "breadth", "accuracy", "none"
    priority_urls: List[str]
    reasoning: str
    quality_gaps: List[str]


class ContentQualitySchema(BaseModel):
    """å†…å®¹è´¨é‡è¯„ä¼°æ¶æ„"""
    overall_score: float = Field(description="æ€»ä½“è´¨é‡è¯„åˆ† 0-1")
    needs_enhancement: bool = Field(description="æ˜¯å¦éœ€è¦å¢å¼º")
    enhancement_type: str = Field(description="å¢å¼ºç±»å‹ï¼šdepth/breadth/accuracy/none")
    priority_urls: List[str] = Field(description="ä¼˜å…ˆå¢å¼ºçš„URLåˆ—è¡¨")
    quality_gaps: List[str] = Field(description="è´¨é‡ç¼ºå£åˆ—è¡¨")
    reasoning: str = Field(description="å†³ç­–æ¨ç†è¿‡ç¨‹")


class ContentEnhancer:
    """å†…å®¹å¢å¼ºå™¨"""
    
    def __init__(self):
        self.llm = None
    
    def get_llm(self, config: RunnableConfig = None) -> ChatGoogleGenerativeAI:
        """è·å–LLMå®ä¾‹"""
        if not self.llm:
            self.llm = ChatGoogleGenerativeAI(
                model="gemini-1.5-flash",
                temperature=0.2,  # è¯„ä¼°éœ€è¦è¾ƒä½æ¸©åº¦
                max_retries=2,
                api_key=os.getenv("GEMINI_API_KEY"),
            )
        return self.llm
    
    def extract_grounding_sources(self, state: AdvancedResearchState) -> List[Dict[str, str]]:
        """ä»ç ”ç©¶çŠ¶æ€ä¸­æå–ä¿¡æ¯æº"""
        grounding_sources = []
        
        # ä»æœç´¢ç»“æœä¸­æå–
        search_results = state.get("web_research_results", [])
        for result in search_results[-10:]:  # æœ€è¿‘10ä¸ªç»“æœ
            if isinstance(result, dict):
                # å…¼å®¹ä¸åŒçš„URLå­—æ®µåï¼š'url' æˆ– 'link'ï¼ˆGoogleæœç´¢ä½¿ç”¨'link'ï¼‰
                result_url = result.get("url") or result.get("link", "")
                grounding_sources.append({
                    "title": result.get("title", "æœªçŸ¥æ ‡é¢˜"),
                    "url": result_url,
                    "snippet": result.get("snippet", result.get("content", ""))[:200]  # é™åˆ¶é•¿åº¦
                })
        
        # ä»æ”¶é›†çš„æºä¸­æå–
        sources_gathered = state.get("sources_gathered", [])
        for source in sources_gathered[-5:]:  # æœ€è¿‘5ä¸ªæº
            # å…¼å®¹ä¸åŒçš„URLå­—æ®µå
            source_url = source.get("url") or source.get("link", "") if isinstance(source, dict) else ""
            if isinstance(source, dict) and source_url:
                grounding_sources.append({
                    "title": source.get("title", "æœªçŸ¥æ¥æº"),
                    "url": source_url,
                    "snippet": source.get("snippet", "")[:200]
                })
        
        # å»é‡ï¼ˆåŸºäºURLï¼‰
        seen_urls = set()
        unique_sources = []
        for source in grounding_sources:
            url = source.get("url", "")
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_sources.append(source)
        
        return unique_sources[:8]  # æœ€å¤š8ä¸ªæº
    
    def generate_enhancement_prompt(
        self, 
        research_topic: str,
        current_findings: List[str],
        grounding_sources: List[Dict[str, str]]
    ) -> str:
        """ç”Ÿæˆå†…å®¹å¢å¼ºè¯„ä¼°æç¤º"""
        
        findings_text = "\n".join([f"- {finding}" for finding in current_findings[-5:]])
        sources_text = "\n".join([
            f"- [{source['title']}]({source['url']}): {source['snippet']}"
            for source in grounding_sources
        ])
        
        return f"""ä½ æ˜¯ **ContentEnhancementAnalyzer**ã€‚ä½ çš„ä»»åŠ¡æ˜¯è¯„ä¼°å½“å‰ç ”ç©¶å†…å®¹çš„è´¨é‡ï¼Œå¹¶å†³å®šæ˜¯å¦éœ€è¦æ·±åº¦å†…å®¹å¢å¼ºã€‚

=== ç ”ç©¶ä¸»é¢˜ ===
{research_topic}

=== å½“å‰ç ”ç©¶å‘ç° ===
{findings_text}

=== å¯ç”¨ä¿¡æ¯æº ===
{sources_text}

=== è¯„ä¼°æ ‡å‡† ===
åŸºäºä»¥ä¸‹ç»´åº¦è¯„ä¼°å†…å®¹è´¨é‡ï¼š

1. **å®Œæ•´æ€§** (0-1)ï¼šæ˜¯å¦æ¶µç›–äº†ç ”ç©¶ä¸»é¢˜çš„ä¸»è¦æ–¹é¢
2. **æ·±åº¦** (0-1)ï¼šæ˜¯å¦æä¾›äº†è¶³å¤Ÿçš„æŠ€æœ¯ç»†èŠ‚å’Œæ·±å…¥åˆ†æ
3. **å‡†ç¡®æ€§** (0-1)ï¼šä¿¡æ¯æ˜¯å¦æ¥è‡ªå¯é æºï¼Œæ˜¯å¦æœ‰æ˜ç¡®å¼•ç”¨
4. **æ—¶æ•ˆæ€§** (0-1)ï¼šä¿¡æ¯æ˜¯å¦ä¸ºæœ€æ–°ï¼Œæ˜¯å¦åŒ…å«å½“å‰è¶‹åŠ¿
5. **å¤šæ ·æ€§** (0-1)ï¼šæ˜¯å¦ä»å¤šä¸ªè§’åº¦å’Œæ¥æºæ”¶é›†äº†ä¿¡æ¯

=== å¢å¼ºç±»å‹å®šä¹‰ ===
- **depth**ï¼šéœ€è¦æ·±å…¥æŠ€æœ¯ç»†èŠ‚ã€å®ç°æ–¹å¼ã€å…·ä½“æ¡ˆä¾‹
- **breadth**ï¼šéœ€è¦æ‰©å±•è¦†ç›–èŒƒå›´ã€æ›´å¤šè§’åº¦ã€ç›¸å…³é¢†åŸŸ
- **accuracy**ï¼šéœ€è¦éªŒè¯ä¿¡æ¯ã€è·å–æƒå¨æ¥æºã€æ›´æ–°æ•°æ®
- **none**ï¼šå½“å‰å†…å®¹è´¨é‡å……è¶³ï¼Œæ— éœ€å¢å¼º

=== å†³ç­–é€»è¾‘ ===
**éœ€è¦å¢å¼ºçš„æƒ…å†µ**ï¼š
- æ€»ä½“è´¨é‡è¯„åˆ† < 0.75
- å…³é”®ç»´åº¦è¯„åˆ† < 0.6
- ç¼ºå°‘æƒå¨æ¥æºæˆ–å®˜æ–¹æ•°æ®
- ä¿¡æ¯è¿‡äºè¡¨é¢ï¼Œç¼ºä¹æ·±åº¦åˆ†æ
- ç ”ç©¶ä¸»é¢˜å¤æ‚ä½†å†…å®¹è¦†ç›–ä¸å…¨

**æ— éœ€å¢å¼ºçš„æƒ…å†µ**ï¼š
- æ€»ä½“è´¨é‡è¯„åˆ† â‰¥ 0.8
- æ‰€æœ‰ç»´åº¦è¯„åˆ† â‰¥ 0.7
- æœ‰è¶³å¤Ÿçš„æ·±åº¦åˆ†æå’Œæƒå¨å¼•ç”¨
- å†…å®¹å…¨é¢è¦†ç›–ç ”ç©¶ä¸»é¢˜

=== è¾“å‡ºæ ¼å¼ ===
è¿”å›JSONæ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š

{{
  "overall_score": 0.0-1.0,
  "needs_enhancement": true/false,
  "enhancement_type": "depth|breadth|accuracy|none",
  "priority_urls": ["url1", "url2", ...],
  "quality_gaps": ["ç¼ºå£1", "ç¼ºå£2", ...],
  "reasoning": "è¯¦ç»†çš„å†³ç­–æ¨ç†è¿‡ç¨‹"
}}

=== æŒ‡ä»¤ ===
ä»”ç»†åˆ†æå½“å‰ç ”ç©¶å†…å®¹çš„è´¨é‡ï¼Œè¯†åˆ«ä¿¡æ¯ç¼ºå£ï¼Œå¹¶ç»™å‡ºæ˜ç¡®çš„å¢å¼ºå»ºè®®ã€‚**åª**è¾“å‡ºJSONæ ¼å¼çš„è¯„ä¼°ç»“æœã€‚"""
    
    def analyze_enhancement_need(
        self,
        research_topic: str,
        current_findings: List[str],
        grounding_sources: List[Dict[str, str]],
        config: RunnableConfig = None
    ) -> EnhancementDecision:
        """åˆ†æå†…å®¹å¢å¼ºéœ€æ±‚"""
        
        # ç”Ÿæˆè¯„ä¼°æç¤º
        prompt = self.generate_enhancement_prompt(
            research_topic, current_findings, grounding_sources
        )
        
        # è·å–ç»“æ„åŒ–è¾“å‡ºLLM
        llm = self.get_llm(config)
        structured_llm = llm.with_structured_output(ContentQualitySchema)
        
        try:
            # è°ƒç”¨LLMè¿›è¡Œè¯„ä¼°
            result = structured_llm.invoke(prompt)
            
            # è½¬æ¢ä¸ºEnhancementDecision
            decision = EnhancementDecision(
                needs_enhancement=result.needs_enhancement,
                confidence_score=result.overall_score,
                enhancement_type=result.enhancement_type,
                priority_urls=result.priority_urls,
                reasoning=result.reasoning,
                quality_gaps=result.quality_gaps
            )
            
            print(f"ğŸ” å†…å®¹è´¨é‡è¯„ä¼°å®Œæˆï¼š")
            print(f"  è´¨é‡è¯„åˆ†ï¼š{decision.confidence_score:.2f}")
            print(f"  éœ€è¦å¢å¼ºï¼š{decision.needs_enhancement}")
            print(f"  å¢å¼ºç±»å‹ï¼š{decision.enhancement_type}")
            print(f"  ä¼˜å…ˆURLæ•°é‡ï¼š{len(decision.priority_urls)}")
            
            return decision
            
        except Exception as e:
            print(f"âŒ å†…å®¹è´¨é‡è¯„ä¼°å¤±è´¥ï¼š{e}")
            
            # fallbackï¼šåŸºäºç®€å•è§„åˆ™
            return self._fallback_analysis(current_findings, grounding_sources)
    
    def _fallback_analysis(
        self, 
        current_findings: List[str], 
        grounding_sources: List[Dict[str, str]]
    ) -> EnhancementDecision:
        """å¤‡ç”¨åˆ†ææ–¹æ³•"""
        
        findings_count = len(current_findings)
        sources_count = len(grounding_sources)
        
        # ç®€å•è§„åˆ™åˆ¤æ–­
        if findings_count < 3 or sources_count < 2:
            needs_enhancement = True
            enhancement_type = "breadth"
            confidence_score = 0.4
        elif findings_count < 5 or sources_count < 4:
            needs_enhancement = True
            enhancement_type = "depth"
            confidence_score = 0.6
        else:
            needs_enhancement = False
            enhancement_type = "none"
            confidence_score = 0.8
        
        priority_urls = [source["url"] for source in grounding_sources[:3]]
        
        return EnhancementDecision(
            needs_enhancement=needs_enhancement,
            confidence_score=confidence_score,
            enhancement_type=enhancement_type,
            priority_urls=priority_urls,
            reasoning="åŸºäºç®€å•è§„åˆ™çš„fallbackåˆ†æ",
            quality_gaps=["ä¿¡æ¯æ•°é‡ä¸è¶³"] if needs_enhancement else []
        )
    
    def execute_enhancement(
        self, 
        priority_urls: List[str], 
        enhancement_type: str,
        config: RunnableConfig = None
    ) -> List[Dict[str, Any]]:
        """æ‰§è¡Œå†…å®¹å¢å¼ºï¼ˆå ä½ç¬¦å®ç°ï¼‰"""
        
        # è¿™é‡Œåº”è¯¥é›†æˆFirecrawlæˆ–å…¶ä»–æ·±åº¦æŠ“å–å·¥å…·
        # ç›®å‰è¿”å›æ¨¡æ‹Ÿç»“æœ
        
        enhanced_results = []
        
        for url in priority_urls[:3]:  # é™åˆ¶å¤„ç†æ•°é‡
            enhanced_result = {
                "url": url,
                "enhanced_content": f"[å¢å¼ºå†…å®¹] å¯¹ {url} çš„æ·±åº¦åˆ†æç»“æœ...",
                "enhancement_type": enhancement_type,
                "extraction_success": True,
                "content_length": 1500,
                "quality_score": 0.85
            }
            enhanced_results.append(enhanced_result)
        
        print(f"ğŸ” å†…å®¹å¢å¼ºå®Œæˆï¼šå¤„ç†äº† {len(enhanced_results)} ä¸ªURL")
        
        return enhanced_results


# å…¨å±€å¢å¼ºå™¨å®ä¾‹
_content_enhancer = ContentEnhancer()


def content_enhancement_node(state: AdvancedResearchState, config: RunnableConfig) -> Dict[str, Any]:
    """
    LangGraphå†…å®¹å¢å¼ºèŠ‚ç‚¹
    æ™ºèƒ½å†³ç­–æ˜¯å¦éœ€è¦æ·±åº¦å†…å®¹æŠ“å–
    """
    
    print(f"ğŸ” å†…å®¹å¢å¼ºå™¨å¯åŠ¨...")
    
    # è·å–å½“å‰ç ”ç©¶ä¸Šä¸‹æ–‡
    current_task = get_current_task(state)
    research_topic = current_task.description
    
    # è·å–å½“å‰ç ”ç©¶å‘ç°
    current_findings = state.get("web_research_results", [])
    findings_content = [
        result.get("content", result.get("snippet", ""))
        for result in current_findings
        if isinstance(result, dict)
    ]
    
    # æå–å¼•ç”¨æº
    grounding_sources = _content_enhancer.extract_grounding_sources(state)
    
    print(f"  ç ”ç©¶ä¸»é¢˜ï¼š{research_topic}")
    print(f"  å½“å‰å‘ç°æ•°é‡ï¼š{len(findings_content)}")
    print(f"  å¯ç”¨ä¿¡æ¯æºï¼š{len(grounding_sources)}")
    
    # åˆ†æå¢å¼ºéœ€æ±‚
    decision = _content_enhancer.analyze_enhancement_need(
        research_topic=research_topic,
        current_findings=findings_content,
        grounding_sources=grounding_sources,
        config=config
    )
    
    # æ„å»ºç»“æœ
    result = {
        "enhancement_decision": {
            "needs_enhancement": decision.needs_enhancement,
            "confidence_score": decision.confidence_score,
            "enhancement_type": decision.enhancement_type,
            "priority_urls": decision.priority_urls,
            "reasoning": decision.reasoning,
            "quality_gaps": decision.quality_gaps
        },
        "enhancement_status": "analyzing"
    }
    
    # å¦‚æœéœ€è¦å¢å¼ºï¼Œæ‰§è¡Œå¢å¼º
    if decision.needs_enhancement and decision.priority_urls:
        print(f"ğŸš€ æ‰§è¡Œå†…å®¹å¢å¼ºï¼Œç±»å‹ï¼š{decision.enhancement_type}")
        
        enhanced_results = _content_enhancer.execute_enhancement(
            priority_urls=decision.priority_urls,
            enhancement_type=decision.enhancement_type,
            config=config
        )
        
        result.update({
            "enhanced_content_results": enhanced_results,
            "enhanced_sources_count": len(enhanced_results),
            "enhancement_status": "completed",
            "enhancement_quality_boost": min(0.3, (1.0 - decision.confidence_score) / 2)
        })
        
        print(f"âœ… å¢å¼ºå®Œæˆï¼šè·å¾— {len(enhanced_results)} ä¸ªå¢å¼ºç»“æœ")
        
    else:
        print(f"â­ï¸ è·³è¿‡å†…å®¹å¢å¼ºï¼šè´¨é‡è¯„åˆ† {decision.confidence_score:.2f}")
        result.update({
            "enhancement_status": "skipped",
            "enhanced_sources_count": 0
        })
    
    return result


def should_enhance_content(state: AdvancedResearchState) -> str:
    """
    å†³å®šæ˜¯å¦åº”è¯¥è¿›è¡Œå†…å®¹å¢å¼º
    ç”¨äºLangGraphçš„æ¡ä»¶è¾¹
    """
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¢å¼ºå†³ç­–ç»“æœ
    enhancement_decision = state.get("enhancement_decision")
    
    if enhancement_decision and enhancement_decision.get("needs_enhancement", False):
        print("ğŸ” å†…å®¹éœ€è¦å¢å¼ºï¼Œå¯åŠ¨å¢å¼ºæµç¨‹")
        return "analyze_enhancement_need"
    else:
        print("â­ï¸ å†…å®¹è´¨é‡å……è¶³ï¼Œè·³è¿‡å¢å¼º")
        return "continue_without_enhancement" 