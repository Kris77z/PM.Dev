"""
V2é«˜çº§APIå·¥å…·é›†
åŒ…å«APIé‡è¯•æœºåˆ¶ã€å¹¶è¡Œå¤„ç†ã€Firecrawlé›†æˆç­‰æ ¸å¿ƒåŠŸèƒ½
"""

import asyncio
import time
import logging
from typing import Optional, Dict, Any, List, Callable, Awaitable
from dataclasses import dataclass
from contextlib import asynccontextmanager
import aiohttp
from concurrent.futures import ThreadPoolExecutor
import json
from dotenv import load_dotenv
import os
from urllib.parse import urlparse

logger = logging.getLogger(__name__)

# V2 Firecrawlé…ç½®ç±» - åŸºäºFiresearchæœ€ä½³å®è·µ
@dataclass
class V2FirecrawlConfig:
    """V2 Firecrawlé…ç½®å‚æ•°"""
    # é€Ÿç‡é™åˆ¶ - æ›´ä¿å®ˆçš„è®¾ç½®
    MAX_CONCURRENT_REQUESTS: int = 1        # V2é™ä½å¹¶å‘æ•°
    MAX_SOURCES_TO_SCRAPE: int = 2          # V2å‡å°‘æŠ“å–æ•°é‡
    REQUEST_DELAY: float = 2.0              # V2å¢åŠ è¯·æ±‚é—´å»¶è¿Ÿ
    
    # é‡è¯•é…ç½®
    MAX_RETRIES: int = 1                    # V2å‡å°‘é‡è¯•æ¬¡æ•°
    RETRY_DELAY: float = 3.0                # V2å¢åŠ é‡è¯•å»¶è¿Ÿ
    
    # è¶…æ—¶é…ç½®
    SCRAPE_TIMEOUT: int = 10                # V2ç¼©çŸ­è¶…æ—¶æ—¶é—´
    BATCH_TIMEOUT: int = 20                 # V2ç¼©çŸ­æ‰¹é‡è¶…æ—¶
    
    # å†…å®¹é™åˆ¶
    MIN_CONTENT_LENGTH: int = 50            # V2é™ä½æœ€å°é•¿åº¦è¦æ±‚
    MAX_CONTENT_LENGTH: int = 3000          # V2å‡å°‘æœ€å¤§å†…å®¹é•¿åº¦
    
    # è´¨é‡æ§åˆ¶
    PRIORITY_DOMAINS: List[str] = None
    EXCLUDED_DOMAINS: List[str] = None
    
    def __post_init__(self):
        if self.PRIORITY_DOMAINS is None:
            self.PRIORITY_DOMAINS = [
                '.edu', '.gov', '.org',
                'wikipedia.org', 'arxiv.org'
            ]
        
        if self.EXCLUDED_DOMAINS is None:
            self.EXCLUDED_DOMAINS = [
                'twitter.com', 'facebook.com', 'instagram.com',
                'tiktok.com', 'youtube.com', 'linkedin.com',
                'pinterest.com', 'reddit.com'
            ]

# V2å…¨å±€é…ç½®å®ä¾‹
V2_FIRECRAWL_CONFIG = V2FirecrawlConfig()

@dataclass
class APICallResult:
    """APIè°ƒç”¨ç»“æœ"""
    success: bool
    data: Any = None
    error: str = ""
    attempt_count: int = 0
    execution_time: float = 0.0

class APIRetryManager:
    """APIé‡è¯•ç®¡ç†å™¨ - è§£å†³è¶…æ—¶å’Œå¤±è´¥é—®é¢˜"""
    
    def __init__(self, max_retries: int = 3, base_delay: float = 1.0, timeout: float = 30.0):
        self.max_retries = max_retries
        self.base_delay = base_delay
        self.timeout = timeout
        self.call_stats = {
            "total_calls": 0,
            "successful_calls": 0,
            "failed_calls": 0,
            "retry_calls": 0
        }
    
    async def robust_api_call(self, 
                            api_func: Callable[..., Awaitable[Any]], 
                            *args, 
                            **kwargs) -> APICallResult:
        """å¸¦é‡è¯•æœºåˆ¶çš„APIè°ƒç”¨"""
        start_time = time.time()
        last_error = None
        
        for attempt in range(self.max_retries):
            try:
                self.call_stats["total_calls"] += 1
                if attempt > 0:
                    self.call_stats["retry_calls"] += 1
                
                logger.info(f"ğŸ”„ APIè°ƒç”¨å°è¯• {attempt + 1}/{self.max_retries}")
                
                # ä½¿ç”¨è¶…æ—¶æ§åˆ¶
                result = await asyncio.wait_for(
                    api_func(*args, **kwargs), 
                    timeout=self.timeout
                )
                
                execution_time = time.time() - start_time
                self.call_stats["successful_calls"] += 1
                
                logger.info(f"âœ… APIè°ƒç”¨æˆåŠŸï¼Œå°è¯•æ¬¡æ•°: {attempt + 1}, è€—æ—¶: {execution_time:.2f}s")
                
                return APICallResult(
                    success=True,
                    data=result,
                    attempt_count=attempt + 1,
                    execution_time=execution_time
                )
                
            except asyncio.TimeoutError as e:
                last_error = f"è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1})"
                logger.warning(f"â° {last_error}")
                
            except Exception as e:
                last_error = f"APIè°ƒç”¨å¤±è´¥: {str(e)} (å°è¯• {attempt + 1})"
                logger.warning(f"âŒ {last_error}")
            
            # æŒ‡æ•°é€€é¿ç­–ç•¥
            if attempt < self.max_retries - 1:
                delay = self.base_delay * (2 ** attempt)
                logger.info(f"â³ ç­‰å¾… {delay} ç§’åé‡è¯•...")
                await asyncio.sleep(delay)
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        execution_time = time.time() - start_time
        self.call_stats["failed_calls"] += 1
        
        fallback_result = f"APIè°ƒç”¨å¤±è´¥ï¼Œå·²é‡è¯•{self.max_retries}æ¬¡ã€‚æœ€åé”™è¯¯: {last_error}"
        logger.error(f"ğŸ’¥ {fallback_result}")
        
        return APICallResult(
            success=False,
            error=fallback_result,
            attempt_count=self.max_retries,
            execution_time=execution_time
        )
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–è°ƒç”¨ç»Ÿè®¡ä¿¡æ¯"""
        total = self.call_stats["total_calls"]
        success_rate = (self.call_stats["successful_calls"] / total * 100) if total > 0 else 0
        
        return {
            **self.call_stats,
            "success_rate": success_rate,
            "failure_rate": 100 - success_rate
        }

class ParallelProcessor:
    """å¹¶è¡Œå¤„ç†å™¨ - æ”¯æŒå¤šä»»åŠ¡å¹¶å‘æ‰§è¡Œ"""
    
    def __init__(self, max_concurrent: int = 5):
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
    
    async def parallel_execute(self, 
                             tasks: List[Callable[..., Awaitable[Any]]], 
                             *args_list, 
                             **kwargs) -> List[APICallResult]:
        """å¹¶è¡Œæ‰§è¡Œå¤šä¸ªä»»åŠ¡"""
        logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œæ‰§è¡Œ {len(tasks)} ä¸ªä»»åŠ¡")
        
        async def execute_single_task(task, task_args):
            async with self.semaphore:
                try:
                    result = await task(*task_args, **kwargs)
                    return APICallResult(success=True, data=result)
                except Exception as e:
                    logger.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}")
                    return APICallResult(success=False, error=str(e))
        
        # å‡†å¤‡ä»»åŠ¡å‚æ•°
        if not args_list:
            args_list = [() for _ in tasks]
        elif len(args_list) != len(tasks):
            args_list = [args_list[0] if args_list else () for _ in tasks]
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        results = await asyncio.gather(
            *[execute_single_task(task, args) for task, args in zip(tasks, args_list)],
            return_exceptions=True
        )
        
        # å¤„ç†å¼‚å¸¸ç»“æœ
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append(
                    APICallResult(success=False, error=str(result))
                )
            else:
                processed_results.append(result)
        
        successful_count = sum(1 for r in processed_results if r.success)
        logger.info(f"âœ… å¹¶è¡Œæ‰§è¡Œå®Œæˆ: {successful_count}/{len(tasks)} æˆåŠŸ")
        
        return processed_results

class RealFirecrawlClient:
    """çœŸå®çš„Firecrawlå®¢æˆ·ç«¯ - V2ç‰ˆæœ¬ï¼Œæ”¯æŒè‡ªåŠ¨é™çº§åˆ°Mock"""
    
    def __init__(self, api_key: str = None):
        from firecrawl import AsyncFirecrawlApp
        
        self.api_key = api_key or os.getenv("FIRECRAWL_API_KEY")
        self.config = V2_FIRECRAWL_CONFIG
        self.last_request_time = 0  # ç”¨äºé€Ÿç‡é™åˆ¶
        
        if self.api_key:
            try:
                self.client = AsyncFirecrawlApp(api_key=self.api_key)
                self.mock_mode = False
                logger.info("ğŸ”¥ V2çœŸå®Firecrawlå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ V2 Firecrawlå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                self.client = None
                self.mock_mode = True
        else:
            logger.info("â„¹ï¸ V2 Firecrawl API Keyæœªé…ç½®ï¼Œä½¿ç”¨Mockæ¨¡å¼")
            self.client = None
            self.mock_mode = True
        
        self.retry_manager = APIRetryManager(
            max_retries=self.config.MAX_RETRIES, 
            timeout=float(self.config.SCRAPE_TIMEOUT)
        )
    
    def _extract_domain(self, url: str) -> str:
        """æå–åŸŸå"""
        try:
            return urlparse(url).netloc.lower()
        except:
            return ""
    
    def _is_suitable_for_enhancement(self, url: str) -> bool:
        """åˆ¤æ–­URLæ˜¯å¦é€‚åˆå¢å¼º"""
        if not url:
            return False
        
        domain = self._extract_domain(url)
        
        # æ£€æŸ¥æ’é™¤åŸŸå
        for excluded in self.config.EXCLUDED_DOMAINS:
            if excluded in domain:
                return False
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹æ’é™¤
        excluded_extensions = ['.pdf', '.doc', '.docx', '.ppt', '.pptx']
        if any(url.lower().endswith(ext) for ext in excluded_extensions):
            return False
        
        return True
    
    def _calculate_priority_score(self, url: str) -> float:
        """è®¡ç®—URLä¼˜å…ˆçº§åˆ†æ•°"""
        domain = self._extract_domain(url)
        score = 0.5
        
        # ä¼˜å…ˆåŸŸååŠ åˆ†
        for priority in self.config.PRIORITY_DOMAINS:
            if priority in domain:
                score += 0.4
                break
        
        # HTTPSåŠ åˆ†
        if url.startswith('https://'):
            score += 0.1
        
        return min(1.0, max(0.0, score))
    
    async def _rate_limited_request(self):
        """å®ç°é€Ÿç‡é™åˆ¶"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.config.REQUEST_DELAY:
            sleep_time = self.config.REQUEST_DELAY - time_since_last
            logger.debug(f"â±ï¸ V2é€Ÿç‡é™åˆ¶ï¼šç­‰å¾… {sleep_time:.1f} ç§’")
            await asyncio.sleep(sleep_time)
        
        self.last_request_time = time.time()
    
    async def scrape_url(self, url: str, options: Dict[str, Any] = None) -> Dict[str, Any]:
        """æŠ“å–å•ä¸ªURLçš„æ·±åº¦å†…å®¹"""
        
        if self.mock_mode:
            return await self._mock_scrape_url(url, options)
        
        async def _real_scrape():
            # æ„å»ºFirecrawlå‚æ•° - æ­£ç¡®çš„å‚æ•°ä¼ é€’æ–¹å¼
            scrape_kwargs = {
                "formats": ["markdown", "html"],
                "only_main_content": True,
                "timeout": 20000
            }
            
            # åˆå¹¶ç”¨æˆ·é€‰é¡¹
            if options:
                scrape_kwargs.update(options)
            
            # è°ƒç”¨çœŸå®çš„Firecrawl API - ä½¿ç”¨æ­£ç¡®çš„å‚æ•°ä¼ é€’æ–¹å¼
            result = await self.client.scrape_url(url, **scrape_kwargs)
            
            # å¤„ç†Firecrawl SDKè¿”å›çš„å¯¹è±¡
            if result and hasattr(result, 'success') and result.success:
                # ä»ScrapeResponseå¯¹è±¡ä¸­æå–æ•°æ®
                metadata = getattr(result, 'metadata', {}) or {}
                
                # è·å–å†…å®¹
                markdown_content = getattr(result, 'markdown', '')
                html_content = getattr(result, 'html', '')
                content = markdown_content or html_content or ''
                
                return {
                    "success": True,
                    "url": url,
                    "title": metadata.get("title", f"Content from {url}") if isinstance(metadata, dict) else f"Content from {url}",
                    "content": content,
                    "html": html_content,
                    "metadata": metadata if isinstance(metadata, dict) else {},
                    "word_count": len(content.split()) if content else 0,
                    "extraction_time": time.time(),
                    "source": "firecrawl_real"
                }
            else:
                # å¤„ç†é”™è¯¯æƒ…å†µ
                error_msg = "Unknown Firecrawl error"
                if result:
                    if hasattr(result, 'error'):
                        error_msg = str(result.error)
                    elif hasattr(result, 'success') and not result.success:
                        error_msg = "Firecrawl scraping failed"
                else:
                    error_msg = "No response from Firecrawl"
                raise Exception(error_msg)
        
        result = await self.retry_manager.robust_api_call(_real_scrape)
        
        if result.success:
            logger.info(f"ğŸ”¥ FirecrawlæŠ“å–æˆåŠŸ: {url} ({result.data.get('word_count', 0)} è¯)")
            return result.data
        else:
            logger.warning(f"ğŸ”¥ FirecrawlæŠ“å–å¤±è´¥: {url} - {result.error}")
            # ä¸ä½¿ç”¨å‡æ•°æ®ï¼Œç›´æ¥è¿”å›å¤±è´¥çŠ¶æ€
            return {
                "success": False,
                "url": url,
                "title": "å¢å¼ºå¤±è´¥",
                "content": "",  # ç©ºå†…å®¹ï¼Œä¸æ±¡æŸ“æŠ¥å‘Š
                "error": result.error,
                "source": "firecrawl_failed"
            }
    
    async def _mock_scrape_url(self, url: str, options: Dict[str, Any] = None) -> Dict[str, Any]:
        """MockæŠ“å–æ¨¡å¼ - é™çº§æ–¹æ¡ˆ"""
        
        async def _mock_scrape():
            # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
            await asyncio.sleep(0.5)
            
            # æ¨¡æ‹Ÿä¸åŒçš„æŠ“å–ç»“æœ
            if "example.com" in url:
                return {
                    "success": True,
                    "url": url,
                    "title": f"æ·±åº¦å†…å®¹æ ‡é¢˜ - {url}",
                    "content": f"è¿™æ˜¯ä» {url} æŠ“å–çš„æ·±åº¦å†…å®¹ã€‚åŒ…å«è¯¦ç»†çš„æŠ€æœ¯åˆ†æã€æ¡ˆä¾‹ç ”ç©¶å’Œä¸“ä¸šè§è§£ã€‚" * 10,
                    "metadata": {"author": "ä¸“å®¶ä½œè€…", "publish_date": "2024-01-01"},
                    "word_count": 500,
                    "extraction_time": time.time(),
                    "source": "firecrawl_mock"
                }
            else:
                # æ¨¡æ‹ŸçœŸå®çš„å†…å®¹æŠ“å–
                return {
                    "success": True,
                    "url": url,
                    "title": f"å¢å¼ºå†…å®¹ - {url.split('/')[-1]}",
                    "content": f"ä» {url} è·å–çš„å¢å¼ºå†…å®¹ï¼š\n\n" + 
                              "è¿™æ˜¯é€šè¿‡Firecrawlæ·±åº¦æŠ“å–è·å¾—çš„é«˜è´¨é‡å†…å®¹ã€‚" * 20,
                    "metadata": {"source": "firecrawl_mock", "quality": "high"},
                    "word_count": 800,
                    "extraction_time": time.time(),
                    "source": "firecrawl_mock"
                }
        
        result = await self.retry_manager.robust_api_call(_mock_scrape)
        
        if result.success:
            logger.info(f"ğŸ”¥ Mock FirecrawlæŠ“å–æˆåŠŸ: {url} ({result.data.get('word_count', 0)} è¯)")
            return result.data
        else:
            logger.warning(f"ğŸ”¥ Mock FirecrawlæŠ“å–å¤±è´¥: {url} - {result.error}")
            return {
                "success": False,
                "url": url,
                "title": "æŠ“å–å¤±è´¥",
                "content": f"æ— æ³•æŠ“å–å†…å®¹: {result.error}",
                "error": result.error,
                "source": "firecrawl_mock"
            }
    
    async def batch_scrape(self, urls: List[str], options: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """æ‰¹é‡æŠ“å–å¤šä¸ªURL"""
        logger.info(f"ğŸ”¥ å¼€å§‹æ‰¹é‡FirecrawlæŠ“å–: {len(urls)} ä¸ªURL (Real: {not self.mock_mode})")
        
        if not self.mock_mode and self.client and len(urls) > 1:
            # å°è¯•ä½¿ç”¨Firecrawlçš„æ‰¹é‡æŠ“å–åŠŸèƒ½
            try:
                batch_kwargs = {
                    "formats": ["markdown", "html"],
                    "only_main_content": True,
                    "timeout": 20000
                }
                
                if options:
                    batch_kwargs.update(options)
                
                batch_result = await self.client.batch_scrape_urls(urls, **batch_kwargs)
                
                # å¤„ç†æ‰¹é‡æŠ“å–çš„å“åº”å¯¹è±¡
                if batch_result and hasattr(batch_result, 'success') and batch_result.success:
                    scraped_data = []
                    # æ‰¹é‡æŠ“å–é€šå¸¸è¿”å›çš„æ˜¯å“åº”å¯¹è±¡ï¼Œéœ€è¦è·å–æ•°æ®åˆ—è¡¨
                    data_list = getattr(batch_result, 'data', []) or []
                    
                    for item in data_list:
                        if item:
                            # å¤„ç†å•ä¸ªæŠ“å–ç»“æœ
                            metadata = getattr(item, 'metadata', {}) or {}
                            markdown_content = getattr(item, 'markdown', '')
                            html_content = getattr(item, 'html', '')
                            content = markdown_content or html_content or ''
                            
                            scraped_data.append({
                                "success": True,
                                "url": metadata.get("sourceURL", "unknown") if isinstance(metadata, dict) else "unknown",
                                "title": metadata.get("title", "Unknown title") if isinstance(metadata, dict) else "Unknown title",
                                "content": content,
                                "html": html_content,
                                "metadata": metadata if isinstance(metadata, dict) else {},
                                "word_count": len(content.split()) if content else 0,
                                "extraction_time": time.time(),
                                "source": "firecrawl_real_batch"
                            })
                    
                    successful_count = len(scraped_data)
                    logger.info(f"ğŸ”¥ æ‰¹é‡FirecrawlæŠ“å–å®Œæˆ: {successful_count}/{len(urls)} æˆåŠŸ")
                    return scraped_data
                    
            except Exception as e:
                logger.warning(f"ğŸ”¥ æ‰¹é‡FirecrawlæŠ“å–å¤±è´¥ï¼Œé™çº§åˆ°å•ä¸ªæŠ“å–: {e}")
        
        # é™çº§åˆ°å•ä¸ªæŠ“å–æ¨¡å¼
        processor = ParallelProcessor(max_concurrent=3)
        
        # åˆ›å»ºæŠ“å–ä»»åŠ¡
        scrape_tasks = [self.scrape_url for _ in urls]
        args_list = [(url, options) for url in urls]
        
        results = await processor.parallel_execute(scrape_tasks, *args_list)
        
        # æå–æˆåŠŸçš„ç»“æœ
        scraped_data = []
        for result in results:
            if result.success:
                scraped_data.append(result.data)
            else:
                scraped_data.append({
                    "success": False,
                    "url": "unknown",
                    "content": f"æŠ“å–å¤±è´¥: {result.error}",
                    "error": result.error,
                    "source": "firecrawl_fallback"
                })
        
        successful_count = sum(1 for data in scraped_data if data.get("success", False))
        logger.info(f"ğŸ”¥ æ‰¹é‡æŠ“å–å®Œæˆ: {successful_count}/{len(urls)} æˆåŠŸ")
        
        return scraped_data


# ä¿ç•™Mockç±»ä»¥ä¾›å‘åå…¼å®¹
class MockFirecrawlClient(RealFirecrawlClient):
    """å‘åå…¼å®¹çš„Mockå®¢æˆ·ç«¯ - ç°åœ¨åŸºäºçœŸå®å®¢æˆ·ç«¯"""
    
    def __init__(self):
        # å¼ºåˆ¶ä½¿ç”¨Mockæ¨¡å¼
        super().__init__(api_key=None)
        self.mock_mode = True
        self.client = None

# å…¨å±€å®ä¾‹
retry_manager = APIRetryManager()
parallel_processor = ParallelProcessor()
firecrawl_client = RealFirecrawlClient()  # ç°åœ¨ä½¿ç”¨çœŸå®å®¢æˆ·ç«¯ï¼Œä¼šè‡ªåŠ¨é™çº§åˆ°Mockå¦‚æœæ²¡æœ‰API Key

# å·¥å…·å‡½æ•°
async def robust_web_search(search_func: Callable, queries: List[str]) -> List[Dict[str, Any]]:
    """å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæœç´¢æŸ¥è¯¢"""
    logger.info(f"ğŸ” å¼€å§‹å¹¶è¡Œæœç´¢: {len(queries)} ä¸ªæŸ¥è¯¢")
    
    search_tasks = [search_func for _ in queries]
    args_list = [(query,) for query in queries]
    
    results = await parallel_processor.parallel_execute(search_tasks, *args_list)
    
    # åˆå¹¶æ‰€æœ‰æœç´¢ç»“æœ
    all_results = []
    for i, result in enumerate(results):
        if result.success and result.data:
            if isinstance(result.data, list):
                for item in result.data:
                    if isinstance(item, dict):
                        item["source_query"] = queries[i]
                        all_results.append(item)
            elif isinstance(result.data, dict):
                result.data["source_query"] = queries[i]
                all_results.append(result.data)
        else:
            logger.warning(f"ğŸ” æœç´¢å¤±è´¥: {queries[i]} - {result.error}")
    
    logger.info(f"ğŸ” å¹¶è¡Œæœç´¢å®Œæˆ: è·å¾— {len(all_results)} æ¡ç»“æœ")
    return all_results

async def enhance_content_with_firecrawl(search_results: List[Dict], 
                                       enhancement_config: Dict[str, Any] = None) -> List[Dict[str, Any]]:
    """ä½¿ç”¨Firecrawlå¢å¼ºæœç´¢ç»“æœå†…å®¹ - V2ç‰ˆæœ¬ï¼Œä½¿ç”¨ä¸¥æ ¼çš„é€Ÿç‡é™åˆ¶"""
    config = enhancement_config or {}
    firecrawl_config = V2_FIRECRAWL_CONFIG
    
    # ä½¿ç”¨é…ç½®åŒ–çš„é™åˆ¶
    max_urls = config.get("max_urls", firecrawl_config.MAX_SOURCES_TO_SCRAPE)
    quality_threshold = config.get("quality_threshold", 0.6)  # V2é™ä½é˜ˆå€¼
    
    # é€‰æ‹©é«˜è´¨é‡çš„URLè¿›è¡Œå¢å¼º - ä½¿ç”¨ä¼˜å…ˆçº§ç®—æ³•
    priority_urls = []
    url_scores = []
    seen_domains = set()
    
    for result in search_results[:max_urls * 3]:  # æ‰©å¤§å€™é€‰èŒƒå›´
        # å…¼å®¹ä¸åŒçš„URLå­—æ®µåï¼š'url' æˆ– 'link'ï¼ˆGoogleæœç´¢ä½¿ç”¨'link'ï¼‰
        url = result.get("url") or result.get("link", "")
        title = result.get("title", "")
        snippet = result.get("snippet", "")
        
        if not url:
            continue
        
        # åŸŸåå»é‡
        domain = urlparse(url).netloc.lower() if url else ""
        if domain in seen_domains:
            continue
        
        # æ£€æŸ¥æ˜¯å¦é€‚åˆå¢å¼º
        if not _is_url_suitable_for_enhancement(url, firecrawl_config):
            continue
        
        # è®¡ç®—è´¨é‡è¯„åˆ†
        quality_score = _calculate_url_quality_score(url, title, snippet, firecrawl_config)
        
        if quality_score >= quality_threshold:
            url_scores.append((url, quality_score, domain))
            seen_domains.add(domain)
    
    # æŒ‰åˆ†æ•°æ’åºå¹¶é€‰æ‹©å‰Nä¸ª
    url_scores.sort(key=lambda x: x[1], reverse=True)
    priority_urls = [url for url, score, domain in url_scores[:max_urls]]
    
    if not priority_urls:
        logger.info("ğŸ”¥ V2æ²¡æœ‰æ‰¾åˆ°é€‚åˆå¢å¼ºçš„URL")
        return search_results
    
    logger.info(f"ğŸ”¥ V2é€‰æ‹© {len(priority_urls)} ä¸ªURLè¿›è¡ŒFirecrawlå¢å¼º")
    
    # ä½¿ç”¨Firecrawlæ‰¹é‡æŠ“å– - åº”ç”¨é€Ÿç‡é™åˆ¶
    enhanced_data = await firecrawl_client.batch_scrape(priority_urls)
    
    # å°†å¢å¼ºå†…å®¹åˆå¹¶åˆ°æœç´¢ç»“æœä¸­
    enhanced_results = search_results.copy()
    
    for enhanced in enhanced_data:
        if enhanced.get("success") and enhanced.get("content"):
            # åªå¤„ç†çœŸå®çš„å¢å¼ºå†…å®¹ï¼Œè·³è¿‡å¤±è´¥çš„é¡¹
            for result in enhanced_results:
                # å…¼å®¹ä¸åŒçš„URLå­—æ®µåè¿›è¡ŒåŒ¹é…
                result_url = result.get("url") or result.get("link", "")
                if result_url == enhanced.get("url"):
                    # é™åˆ¶å¢å¼ºå†…å®¹é•¿åº¦
                    enhanced_content = enhanced["content"]
                    if len(enhanced_content) > firecrawl_config.MAX_CONTENT_LENGTH:
                        enhanced_content = enhanced_content[:firecrawl_config.MAX_CONTENT_LENGTH] + "..."
                    
                    result["enhanced_content"] = enhanced_content
                    result["enhanced_title"] = enhanced["title"]
                    result["enhanced_metadata"] = enhanced.get("metadata", {})
                    result["word_count"] = enhanced.get("word_count", 0)
                    result["is_enhanced"] = True
                    result["enhancement_source"] = enhanced.get("source", "unknown")
                    break
    
    enhanced_count = sum(1 for r in enhanced_results if r.get("is_enhanced"))
    failed_count = sum(1 for enhanced in enhanced_data if not enhanced.get("success"))
    
    if enhanced_count > 0:
        logger.info(f"ğŸ”¥ V2å†…å®¹å¢å¼ºå®Œæˆ: {enhanced_count} ä¸ªç»“æœè¢«å¢å¼º")
    if failed_count > 0:
        logger.info(f"âš ï¸ V2å†…å®¹å¢å¼ºè·³è¿‡: {failed_count} ä¸ªURLå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æœç´¢ç»“æœ")
    
    return enhanced_results 

def _is_url_suitable_for_enhancement(url: str, config: V2FirecrawlConfig) -> bool:
    """åˆ¤æ–­URLæ˜¯å¦é€‚åˆå¢å¼º"""
    if not url:
        return False
    
    domain = urlparse(url).netloc.lower()
    
    # æ£€æŸ¥æ’é™¤åŸŸå
    for excluded in config.EXCLUDED_DOMAINS:
        if excluded in domain:
            return False
    
    # æ£€æŸ¥æ–‡ä»¶ç±»å‹æ’é™¤
    excluded_extensions = ['.pdf', '.doc', '.docx', '.ppt', '.pptx']
    if any(url.lower().endswith(ext) for ext in excluded_extensions):
        return False
    
    return True

def _calculate_url_quality_score(url: str, title: str, snippet: str, config: V2FirecrawlConfig) -> float:
    """è®¡ç®—URLè´¨é‡åˆ†æ•°"""
    domain = urlparse(url).netloc.lower()
    score = 0.3  # åŸºç¡€åˆ†æ•°
    
    # æ ‡é¢˜é•¿åº¦åŠ åˆ†
    if len(title) > 10:
        score += 0.2
    
    # æ‘˜è¦é•¿åº¦åŠ åˆ†
    if len(snippet) > 50:
        score += 0.2
    
    # ä¼˜å…ˆåŸŸåå¤§å¹…åŠ åˆ†
    for priority in config.PRIORITY_DOMAINS:
        if priority in domain:
            score += 0.4
            break
    
    # HTTPSåŠ åˆ†
    if url.startswith('https://'):
        score += 0.1
    
    return min(1.0, max(0.0, score)) 